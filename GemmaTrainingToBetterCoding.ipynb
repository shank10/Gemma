{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shank10/Gemma/blob/main/GemmaTrainingToBetterCoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments,EvalPrediction,pipeline\n",
        "from peft import LoraConfig, TaskType, get_peft_model , PeftModel, LoraModel\n",
        "from accelerate.utils import release_memory\n",
        "from accelerate import Accelerator\n",
        "from datasets import Dataset\n",
        "import evaluate\n",
        "import accelerate\n",
        "\n",
        "from sklearn.metrics import f1_score\n",
        "from typing import Dict\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_colwidth', None)\n",
        "\n",
        "from trl import SFTTrainer\n",
        "\n",
        "import torch\n",
        "import torch.nn\n",
        "\n",
        "import os\n",
        "os.environ['TOKENIZERS_PARALLELISM'] = \"FALSE\"\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import wandb\n",
        "import gc\n",
        "gc.collect()\n",
        "wandb.init()"
      ],
      "metadata": {
        "id": "_nb29bwK2vtL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "code = pd.read_csv(\"/kaggle/input/python-code-instruction-dataset/train.csv\")\n",
        "code = code.sample(frac = 0.1,random_state = 101,ignore_index=True)\n",
        "qa = pd.read_csv(\"/kaggle/input/glaive-python-code-qa-dataset/train.csv\",names = ['output','instruction'])\n",
        "qa = qa.sample(frac = 0.01,random_state = 101,ignore_index=True)\n",
        "\n",
        "code.info()"
      ],
      "metadata": {
        "id": "vXJFI0XM3VtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "kaggle datasets download -d thedevastator/python-code-instruction-dataset"
      ],
      "metadata": {
        "id": "-y_72BZ9861S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(example):\n",
        "    template = f\"\"\"Instruction:\\n{example['instruction']}\\n\\nResponse:\\n{example['output']}\"\"\"\n",
        "    return template\n",
        "\n",
        "code['template'] = code.apply(preprocess_data,axis=1)\n",
        "qa['template'] = qa.apply(preprocess_data,axis=1)\n",
        "\n",
        "dataset = Dataset.from_pandas(pd.concat([code[['template']],qa[['template']]])).train_test_split(test_size = 0.2)\n",
        "\n",
        "# Example Printing\n",
        "print(code['template'].iloc[1])"
      ],
      "metadata": {
        "id": "A3aSJPCq3fOD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_structure = \"\"\"This is the structure of Gemma Model,\n",
        "\n",
        "\n",
        "GemmaForCausalLM(\n",
        "  (model): GemmaModel(\n",
        "    (embed_tokens): Embedding(256000, 2048, padding_idx=0)\n",
        "    (layers): ModuleList(\n",
        "      (0-17): 18 x GemmaDecoderLayer(\n",
        "        (self_attn): GemmaSdpaAttention(\n",
        "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
        "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
        "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=False)\n",
        "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
        "          (rotary_emb): GemmaRotaryEmbedding()\n",
        "        )\n",
        "        (mlp): GemmaMLP(\n",
        "          (gate_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
        "          (up_proj): Linear4bit(in_features=2048, out_features=16384, bias=False)\n",
        "          (down_proj): Linear4bit(in_features=16384, out_features=2048, bias=False)\n",
        "          (act_fn): GELUActivation()\n",
        "        )\n",
        "        (input_layernorm): GemmaRMSNorm()\n",
        "        (post_attention_layernorm): GemmaRMSNorm()\n",
        "      )\n",
        "    )\n",
        "    (norm): GemmaRMSNorm()\n",
        "  )\n",
        "  (lm_head): Linear(in_features=2048, out_features=256000, bias=False)\n",
        ")\"\"\"\n",
        "print(model_structure)"
      ],
      "metadata": {
        "id": "FV-lIL7i3qbo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = \"/kaggle/input/gemma/transformers/2b-it/2\"\n",
        "lora_config = LoraConfig(\n",
        "        lora_dropout=0.1,\n",
        "        r=8,\n",
        "        target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "        task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_quant_type='fp4',\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "tokenizer.padding_side = \"right\"\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model, quantization_config=bnb_config)\n",
        "\n",
        "loraModel = LoraModel(model= model,config =lora_config,adapter_name = \"adapter\")"
      ],
      "metadata": {
        "id": "zqtqCWow39Rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "print_trainable_parameters(loraModel)"
      ],
      "metadata": {
        "id": "B79Yq_P_4HW0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.empty_cache()\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "oEycdGXa4NZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_adapter_path = \"LoraAdapter\"\n",
        "trainer.model.save_pretrained(lora_adapter_path)\n"
      ],
      "metadata": {
        "id": "47_jE6G24Wgt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Release Memory\n",
        "trainer, model, = release_memory(trainer, model)\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n"
      ],
      "metadata": {
        "id": "6G1mNs5h4b7F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_model = \"Final_Model\"\n",
        "# Loading Base Model\n",
        "model = AutoModelForCausalLM.from_pretrained(base_model,device_map='auto', torch_dtype=torch.float16)\n",
        "\n",
        "# Lodaing  Base Model with LoRA adapters.\n",
        "peft_model = PeftModel.from_pretrained(model,lora_adapter_path,device_map='auto', torch_dtype=torch.float16)\n",
        "\n",
        "# Merging and Saving Model\n",
        "model = peft_model.merge_and_unload(progressbar = True)\n",
        "model.save_pretrained(final_model)\n",
        "tokenizer.save_pretrained(final_model)\n"
      ],
      "metadata": {
        "id": "rZedcwBv4gL5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = release_memory(model)"
      ],
      "metadata": {
        "id": "saPtb_Q54m-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "peft_pipe = pipeline(\"text-generation\",final_model, model_kwargs={\"torch_dtype\": torch.float16},\n",
        "    device_map='auto',\n",
        "    max_new_tokens=512)\n"
      ],
      "metadata": {
        "id": "4Vx8MjZd4r4Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_output(question):\n",
        "    prompt = f\"Provide an optimal Response to the given Instruction, dont include the any kind of extra Explaination in response. keep in short and simple.\\n\\nInstruction:\\n{question}\\n\\nResponse:\\n\"\n",
        "    out = peft_pipe(prompt,\n",
        "     do_sample=True,\n",
        "    temperature=0.1,\n",
        "    top_k=20,\n",
        "    top_p=0.3,\n",
        "    add_special_tokens=True)\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "-h2YqNGv4vhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = get_output(\"Write program to find factorial of number\")\n",
        "print(out[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "JnqrtaVA40CV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = get_output(\"Explain how can an AI classify a given sentence as a declaration, an expression, or a statement?\")\n",
        "print(out[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "X0hX6zDc4755"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = get_output(\"I need to write a bash script that compares two arguments and prints a message if the first argument is greater than the second argument. Can someone help me with the code\")\n",
        "print(out[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "DL1ZA3NA49Me"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "out = get_output(\"How can I optimize a TensorFlow neural network to improve its predictive accuracy?\")\n",
        "print(out[0]['generated_text'])\n"
      ],
      "metadata": {
        "id": "L1pwvAtx5F_4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}