{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsyb2ILvpK+LcwVq1UHAgQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shank10/Gemma/blob/main/TrainGemmaToTranslateinHindi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYt3ir6zqQJD"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata # `userdata` is a Colab API.\n",
        "\n",
        "os.environ[\"KAGGLE_USERNAME\"] = userdata.get('KAGGLE_USERNAME')\n",
        "os.environ[\"KAGGLE_KEY\"] = userdata.get('KAGGLE_KEY')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "By installing gemma, you will also install flax, core jax, optax (the JAX-based gradient processing and optimization library), orbax, and sentencepiece.\n",
        "**Action**: Need to learn flax and jax libraries through different notebook. For the moment its going to be straight-forward APIs."
      ],
      "metadata": {
        "id": "yXmiQ53kq9Rx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the latest version of Jax to support T4 TPU\n",
        "!pip install -U jax[tpu] -f https://storage.googleapis.com/jax-releases/libtpu_releases.html\n",
        "#Jaxlib and Jax versions should be identical\n",
        "!pip list | grep jax\n",
        "!pip install -q git+https://github.com/google-deepmind/gemma.git"
      ],
      "metadata": {
        "id": "hXTu0v_Cq71a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import enum\n",
        "import re\n",
        "import string\n",
        "\n",
        "import chex\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import optax\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "from gemma import params as params_lib\n",
        "from gemma import sampler as sampler_lib\n",
        "from gemma import transformer as transformer_lib\n",
        "import sentencepiece as spm"
      ],
      "metadata": {
        "id": "Z_fe1g9OrIEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GEMMA_VARIANT = '2b-it' # @param ['2b', '2b-it']\n",
        "import kagglehub\n",
        "\n",
        "GEMMA_PATH = kagglehub.model_download(f'google/gemma/flax/{GEMMA_VARIANT}')\n",
        "print('GEMMA_PATH:', GEMMA_PATH)"
      ],
      "metadata": {
        "id": "1ZubHc40tEhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CKPT_PATH = os.path.join(GEMMA_PATH, GEMMA_VARIANT)\n",
        "TOKENIZER_PATH = os.path.join(GEMMA_PATH, 'tokenizer.model')\n",
        "print('CKPT_PATH:', CKPT_PATH)\n",
        "print('TOKENIZER_PATH:', TOKENIZER_PATH)"
      ],
      "metadata": {
        "id": "6yDnhBVJtZgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tfds.load(\"mtnt/en-fr\", split=\"train\")\n",
        "\n",
        "ds = ds.take(2)\n",
        "ds = ds.as_numpy_iterator()\n",
        "\n",
        "for idx, example in enumerate(ds):\n",
        "  print(f'Example {idx}:')\n",
        "  for key, val in example.items():\n",
        "    print(f'{key}: {val}')\n",
        "  print()"
      ],
      "metadata": {
        "id": "Y1XvpP9RQt_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the Gemma tokenizer, constructed using sentencepiece.SentencePieceProcessor\n",
        "vocab = spm.SentencePieceProcessor()\n",
        "vocab.Load(TOKENIZER_PATH)"
      ],
      "metadata": {
        "id": "jweW5YDnQxic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Customize theSentencePieceProcessor for the English-to-French translation task. Since you will be fine-tuning the English portion of the Gemma model, you need to make a few adjustments, such as:\n",
        "\n",
        "The input prefix: Adding a common prefix to each input signals the translation task. For example, you could use a prompt with a prefix like Translate this into French: [INPUT_SENTENCE].\n",
        "\n",
        "The translation start suffix: Adding a suffix at the end of each prompt instructs the Gemma model exactly when to begin the translation process. A new line should do the job.\n",
        "\n",
        "Language model tokens: Gemma models expect a \"beginning of sequence\" token at the beginning of each sequence, so adding an \"end of sequence\" token at the end of each training example should be sufficient.\n",
        "\n"
      ],
      "metadata": {
        "id": "6kxzJM7jRKfp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GemmaTokenizer:\n",
        "\n",
        "  def __init__(self,\n",
        "               spm_processor: spm.SentencePieceProcessor):\n",
        "    self._spm_processor = spm_processor\n",
        "\n",
        "  @property\n",
        "  def pad_id(self) -> int:\n",
        "    \"\"\"Fast access to the pad ID.\"\"\"\n",
        "    return self._spm_processor.pad_id()\n",
        "\n",
        "  def tokenize(self,\n",
        "               example: str | bytes,\n",
        "               prefix: str = '',\n",
        "               suffix: str = '',\n",
        "               add_eos: bool = True) -> jax.Array:\n",
        "    \"\"\"\n",
        "    The tokenization function.\n",
        "\n",
        "    Args:\n",
        "      example: Input string to tokenize.\n",
        "      prefix:  Prefix to add to the input string.\n",
        "      suffix:  Suffix to add to the input string.\n",
        "      add_eos: If True, add an \"end of sentence\" token at the end of the output\n",
        "               sequence.\n",
        "    Returns:\n",
        "      Tokens corresponding to the input string.\n",
        "    \"\"\"\n",
        "    int_list = [self._spm_processor.bos_id()]\n",
        "    int_list.extend(self._spm_processor.EncodeAsIds(prefix + example + suffix))\n",
        "    if add_eos:\n",
        "      int_list.append(self._spm_processor.eos_id())\n",
        "\n",
        "    return jnp.array(int_list, dtype=jnp.int32)\n",
        "\n",
        "  def tokenize_tf_op(self,\n",
        "                     str_tensor: tf.Tensor,\n",
        "                     prefix: str = '',\n",
        "                     suffix: str = '',\n",
        "                     add_eos: bool = True) -> tf.Tensor:\n",
        "    \"\"\"A TensorFlow operator for the tokenize function.\"\"\"\n",
        "    encoded = tf.numpy_function(\n",
        "        self.tokenize,\n",
        "        [str_tensor, prefix, suffix, add_eos],\n",
        "        tf.int32)\n",
        "    encoded.set_shape([None])\n",
        "    return encoded\n",
        "\n",
        "  def to_string(self, tokens: jax.Array) -> str:\n",
        "    \"\"\"Convert an array of tokens to a string.\"\"\"\n",
        "    return self._spm_processor.EncodeIds(tokens.tolist())\n"
      ],
      "metadata": {
        "id": "gqyhpy1fRMiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Try customisation of data on a small sample\n",
        "\n",
        "tokenizer = GemmaTokenizer(vocab)\n",
        "\n",
        "def tokenize_source(tokenizer, example: tf.Tensor):\n",
        "  return tokenizer.tokenize_tf_op(example,\n",
        "                                  prefix='Translate this into French:\\n',\n",
        "                                  suffix='\\n',\n",
        "                                  add_eos=False)\n",
        "def tokenize_destination(tokenizer, example: tf.Tensor):\n",
        "  return tokenizer.tokenize_tf_op(example,\n",
        "                                  add_eos=True)\n",
        "\n",
        "ds = tfds.load(\"mtnt/en-fr\",split=\"train\")\n",
        "ds = ds.take(2)\n",
        "ds = ds.map(lambda x: {'src': tokenize_source(tokenizer, x['src']),\n",
        "                       'dst': tokenize_destination(tokenizer, x['dst'])})\n",
        "ds = ds.as_numpy_iterator()\n",
        "\n",
        "for idx, example in enumerate(ds):\n",
        "  print(f'Example {idx}:')\n",
        "  for key, val in example.items():\n",
        "    print(f'{key}: {val}')\n",
        "  print()\n"
      ],
      "metadata": {
        "id": "lpTLZ9SnRcaV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#If the sample looks okay, run this block to tokenize entire dataset\n",
        "@chex.dataclass(frozen=True)\n",
        "class TrainingInput:\n",
        "  # Input tokens provided to the model.\n",
        "  input_tokens: jax.Array\n",
        "\n",
        "  # A mask that determines which tokens contribute to the target loss\n",
        "  # calculation.\n",
        "  target_mask: jax.Array\n",
        "\n",
        "class DatasetSplit(enum.Enum):\n",
        "  TRAIN = 'train'\n",
        "  VALIDATION = 'valid'\n",
        "\n",
        "class MTNTDatasetBuilder:\n",
        "  \"\"\"The dataset builder for the MTNT dataset.\"\"\"\n",
        "\n",
        "  N_ITEMS = {DatasetSplit.TRAIN: 35_692,\n",
        "             DatasetSplit.VALIDATION: 811}\n",
        "\n",
        "  BUFFER_SIZE_SHUFFLE = 10_000\n",
        "  TRANSLATION_PREFIX = 'Translate this into French:\\n'\n",
        "  TRANSLATION_SUFFIX = '\\n'\n",
        "\n",
        "  def __init__(self,\n",
        "               tokenizer : GemmaTokenizer,\n",
        "               max_seq_len: int):\n",
        "    \"\"\"Constructor.\n",
        "\n",
        "    Args:\n",
        "      tokenizer: Gemma tokenizer to use.\n",
        "      max_seq_len: size of each sequence in a given batch.\n",
        "    \"\"\"\n",
        "    self._tokenizer = tokenizer\n",
        "    self._base_data = {\n",
        "        DatasetSplit.TRAIN: tfds.load(\"mtnt/en-fr\",split=\"train\"),\n",
        "        DatasetSplit.VALIDATION: tfds.load(\"mtnt/en-fr\",split=\"valid\"),\n",
        "    }\n",
        "    self._max_seq_len = max_seq_len\n",
        "\n",
        "  def _tokenize_source(self, example: tf.Tensor):\n",
        "    \"\"\"Tokenization function for the source.\"\"\"\n",
        "    return self._tokenizer.tokenize_tf_op(example,\n",
        "                                          prefix=self.TRANSLATION_PREFIX,\n",
        "                                          suffix=self.TRANSLATION_SUFFIX,\n",
        "                                          add_eos=False)\n",
        "\n",
        "  def _tokenize_destination(self, example: tf.Tensor):\n",
        "    \"\"\"Tokenization function for the French translation.\"\"\"\n",
        "    return self._tokenizer.tokenize_tf_op(example,\n",
        "                                          add_eos=True)\n",
        "\n",
        "  def _pad_up_to_max_len(self,\n",
        "                         input_tensor: tf.Tensor,\n",
        "                         pad_value: int | bool,\n",
        "                         ) -> tf.Tensor:\n",
        "    \"\"\"Pad the given tensor up to sequence length of a batch.\"\"\"\n",
        "    seq_len = tf.shape(input_tensor)[0]\n",
        "    to_pad = tf.maximum(self._max_seq_len - seq_len, 0)\n",
        "    return tf.pad(input_tensor,\n",
        "                  [[0, to_pad]],\n",
        "                  mode='CONSTANT',\n",
        "                  constant_values=pad_value,\n",
        "                  )\n",
        "\n",
        "  def _to_training_input(self,\n",
        "                         src_tokens: jax.Array,\n",
        "                         dst_tokens: jax.Array,\n",
        "                         ) -> TrainingInput:\n",
        "    \"\"\"Build a training input from a tuple of source and destination tokens.\"\"\"\n",
        "\n",
        "    # The input sequence fed to the model is simply the concatenation of the\n",
        "    # source and the destination.\n",
        "    tokens = tf.concat([src_tokens, dst_tokens], axis=0)\n",
        "\n",
        "    # To prevent the model from updating based on the source (input)\n",
        "    # tokens, add a target mask to each input.\n",
        "    q_mask = tf.zeros_like(src_tokens, dtype=tf.bool)\n",
        "    a_mask = tf.ones_like(dst_tokens, dtype=tf.bool)\n",
        "    mask = tf.concat([q_mask, a_mask], axis=0)\n",
        "\n",
        "    # If the output tokens sequence is smaller than the target sequence size,\n",
        "    # then pad it with pad tokens.\n",
        "    tokens = self._pad_up_to_max_len(tokens, self._tokenizer.pad_id)\n",
        "\n",
        "    # Don't want to perform the backward pass on the pad tokens.\n",
        "    mask = self._pad_up_to_max_len(mask, False)\n",
        "\n",
        "    return TrainingInput(input_tokens=tokens, target_mask=mask)\n",
        "\n",
        "\n",
        "  def get_train_dataset(self, batch_size: int, num_epochs: int):\n",
        "    \"\"\"Build the training dataset.\"\"\"\n",
        "\n",
        "    # Tokenize each sample.\n",
        "    ds = self._base_data[DatasetSplit.TRAIN].map(lambda x : (self._tokenize_source(x['src']),\n",
        "                                                             self._tokenize_destination(x['dst'])))\n",
        "\n",
        "    # Convert the samples to training inputs.\n",
        "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
        "\n",
        "    # Remove the samples that are too long.\n",
        "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
        "\n",
        "    # Shuffle the dataset.\n",
        "    ds = ds.shuffle(buffer_size=self.BUFFER_SIZE_SHUFFLE)\n",
        "\n",
        "    # Repeat if necessary.\n",
        "    ds = ds.repeat(num_epochs)\n",
        "\n",
        "    # Build batches.\n",
        "    ds = ds.batch(batch_size, drop_remainder=True)\n",
        "    return ds\n",
        "\n",
        "  def get_validation_dataset(self, batch_size: int):\n",
        "    \"\"\"Build the validation dataset.\"\"\"\n",
        "\n",
        "    # Same steps as in `get_train_dataset`, but without shuffling and no repetition.\n",
        "    ds = self._base_data[DatasetSplit.VALIDATION].map(lambda x : (self._tokenize_source(x['src']),\n",
        "                                                                  self._tokenize_destination(x['dst'])))\n",
        "    ds = ds.map(lambda x, y: self._to_training_input(x, y))\n",
        "    ds = ds.filter(lambda x: tf.shape(x.input_tokens)[0] <= self._max_seq_len)\n",
        "    ds = ds.batch(batch_size, drop_remainder=True)\n",
        "    return ds"
      ],
      "metadata": {
        "id": "2xJnzf3haBCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load and format the Gemma model checkpoint\n",
        "params = params_lib.load_and_format_params(CKPT_PATH)\n",
        "config_2b = transformer_lib.TransformerConfig.from_params(\n",
        "    params,\n",
        "    cache_size=30\n",
        ")\n",
        "\n",
        "model_2b = transformer_lib.Transformer(config=config_2b)\n"
      ],
      "metadata": {
        "id": "Q6bJmGuLbVI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Fine-tune the model\n",
        "In this section, you will:\n",
        "\n",
        "Use the gemma.transformer.Transformer class to create the forward pass and loss function.\n",
        "Build the position and attention mask vectors for tokens\n",
        "Build a training step function with Flax.\n",
        "Build the validation step without the backwards pass.\n",
        "Create the training loop.\n",
        "Fine-tune the Gemma model.\n",
        "Define the forward pass and the loss function using the gemma.transformer.Transformer class. The Gemma Transformer inherits from flax.linen.Module, and offers two essential methods:\n",
        "\n",
        "init: Initializes the model's parameters.\n",
        "apply: Executes the model's __call__ function using a given set of parameters.\n",
        "\n",
        "Since you are working with pre-trained Gemma weights, you don't need to use the init function.\n"
      ],
      "metadata": {
        "id": "VIe0RzK1cgi1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def forward_and_loss_fn(params,\n",
        "                        *,\n",
        "                        model: transformer_lib.Transformer,\n",
        "                        input_tokens: jax.Array,            # Shape [B, L]\n",
        "                        input_mask: jax.Array,              # Shape [B, L]\n",
        "                        positions: jax.Array,               # Shape [B, L]\n",
        "                        attention_mask: jax.Array,          # [B, L, L]\n",
        "                        ) -> jax.Array:\n",
        "  \"\"\"The forward pass and the loss function.\n",
        "\n",
        "  Args:\n",
        "    params: Model's input parameters.\n",
        "    model: The Gemma transformer model to call.\n",
        "    input_tokens: Input tokens sequence, shape [B, L].\n",
        "    input_mask: Tokens to ignore when computing the loss, shape [B, L].\n",
        "    positions: Relative position of each token, shape [B, L].\n",
        "    attention_mask: Input attention mask, shape [B, L].\n",
        "\n",
        "  Returns:\n",
        "    The softmax cross-entropy loss for the next-token prediction task.\n",
        "  \"\"\"\n",
        "\n",
        "  # The forward pass on the input data.\n",
        "  # No attention cache is needed here.\n",
        "  logits, _ = model.apply(\n",
        "        params,\n",
        "        input_tokens,\n",
        "        positions,\n",
        "        None,              # Attention cache is None.\n",
        "        attention_mask,\n",
        "    )\n",
        "\n",
        "  # Exclude the last step as it does not appear in the targets.\n",
        "  logits = logits[0, :-1]\n",
        "\n",
        "  # Similarly, the first token cannot be predicted.\n",
        "  target_tokens = input_tokens[0, 1:]\n",
        "  target_mask = input_mask[0, 1:]\n",
        "\n",
        "  # Convert the target labels to one-hot encoded vectors.\n",
        "  one_hot = jax.nn.one_hot(target_tokens, logits.shape[-1])\n",
        "\n",
        "  # Don't update on unwanted tokens.\n",
        "  one_hot = one_hot * target_mask.astype(one_hot.dtype)[...,None]\n",
        "\n",
        "  # Define the normalization factor.\n",
        "  norm_factor = 1 / (jnp.sum(target_mask) + 1e-8)\n",
        "\n",
        "  # Return the negative log likelihood (NLL) loss.\n",
        "  return -jnp.sum(jax.nn.log_softmax(logits) * one_hot) * norm_factor"
      ],
      "metadata": {
        "id": "GSmjYY7fckoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The gemma.transformer.Transformer class requires an attention_mask and a positions vector alongside each input.\n",
        "#You can generate these by creating a custom function that uses Transformer.build_positions_from_mask and Transformer.make_causal_attn_mask\n",
        "def get_attention_mask_and_positions(example: jax.Array,\n",
        "                                     pad_id : int,\n",
        "                                     )-> tuple[jax.Array, jax.Array]:\n",
        "  \"\"\"Builds the position and attention mask vectors from the given tokens.\"\"\"\n",
        "  pad_mask = example != pad_id\n",
        "  current_token_position = transformer_lib.build_positions_from_mask(pad_mask)\n",
        "  attention_mask = transformer_lib.make_causal_attn_mask(pad_mask)\n",
        "  return current_token_position, attention_mask\n",
        "\n",
        "  # Build the train_step function that performs the backward pass and updates the model's parameters accordingly, where:\n",
        "  # jax.value_and_grad is for evaluating the loss function and gradients during the forward and backward passes.\n",
        "  # optax.apply_updates is for updating the parameters\n",
        "  def train_step(model: transformer_lib.Transformer,\n",
        "               params,\n",
        "               optimizer: optax.GradientTransformation,\n",
        "               opt_state: optax.OptState,\n",
        "               pad_id: int,\n",
        "               example: TrainingInput):\n",
        "  \"\"\"Train step.\n",
        "\n",
        "  Args:\n",
        "    model: The Gemma transformer model.\n",
        "    params: The model's input parameters.\n",
        "    optimizer: The Optax optimizer to use.\n",
        "    opt_state: The input optimizer's state.\n",
        "    pad_id: ID of the pad token.\n",
        "    example: Input batch.\n",
        "\n",
        "  Returns:\n",
        "    The training loss, the updated parameters, and the updated optimizer state.\n",
        "  \"\"\"\n",
        "\n",
        "  # Build the position and attention mask vectors.\n",
        "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
        "\n",
        "  # The forward and backward passes.\n",
        "  train_loss, grads = jax.value_and_grad(forward_and_loss_fn)(params,\n",
        "                                                             model=model,\n",
        "                                                             input_tokens=example.input_tokens,\n",
        "                                                             input_mask=example.target_mask,\n",
        "                                                             positions=positions,\n",
        "                                                             attention_mask=attention_mask)\n",
        "  # Update the parameters.\n",
        "  updates, opt_state = optimizer.update(grads, opt_state)\n",
        "  params = optax.apply_updates(params, updates)\n",
        "\n",
        "  return train_loss, params, opt_state"
      ],
      "metadata": {
        "id": "xti_giAcc3SD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the validation_step function without the backward pass\n",
        "def validation_step(model: transformer_lib.Transformer,\n",
        "                    params,\n",
        "                    pad_id: int,\n",
        "                    example: TrainingInput,\n",
        "                    ):\n",
        "  positions, attention_mask = get_attention_mask_and_positions(example.input_tokens, pad_id)\n",
        "  val_loss = forward_and_loss_fn(params,\n",
        "                                 model=model,\n",
        "                                 input_tokens=example.input_tokens,\n",
        "                                 input_mask=example.target_mask,\n",
        "                                 positions=positions,\n",
        "                                 attention_mask=attention_mask)\n",
        "  return val_loss"
      ],
      "metadata": {
        "id": "QUTupkBid49G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the training loop using optax.sgd for the SGD optimizer\n",
        "@chex.dataclass(frozen=True)\n",
        "class TrainingConfig:\n",
        "  learning_rate: float\n",
        "  num_epochs: int\n",
        "  eval_every_n: int\n",
        "  batch_size: int\n",
        "  max_steps: int | None = None\n",
        "\n",
        "def train_loop(\n",
        "    model: transformer_lib.Transformer,\n",
        "    params,\n",
        "    dataset_builder: MTNTDatasetBuilder,\n",
        "    training_cfg: TrainingConfig):\n",
        "\n",
        "  # Apply `jax.jit` on the training step, making the whole loop much more efficient.\n",
        "  compiled_train_step = jax.jit(train_step, static_argnames=['model', 'optimizer'])\n",
        "\n",
        "  # Apply `jax.jit` on the validation step.\n",
        "  compiled_validation_step = jax.jit(validation_step, static_argnames=['model'])\n",
        "\n",
        "  # To save memory, use the SGD optimizer instead of the usual Adam optimizer.\n",
        "  # Note that for this specific example, SGD is more than enough.\n",
        "  optimizer = optax.sgd(training_cfg.learning_rate)\n",
        "  opt_state = optimizer.init(params)\n",
        "\n",
        "  # Build the training dataset.\n",
        "  train_ds = dataset_builder.get_train_dataset(batch_size=training_cfg.batch_size,\n",
        "                                               num_epochs=training_cfg.num_epochs)\n",
        "  train_ds = train_ds.as_numpy_iterator()\n",
        "\n",
        "  # Build the validation dataset, with a limited number of samples for this demo.\n",
        "  validation_ds = dataset_builder.get_validation_dataset(batch_size=training_cfg.batch_size)\n",
        "  validation_ds = validation_ds.take(50)\n",
        "\n",
        "  n_steps = 0\n",
        "  avg_loss=0\n",
        "\n",
        "  # A first round of the validation loss.\n",
        "  n_steps_eval = 0\n",
        "  eval_loss = 0\n",
        "  val_iterator = validation_ds.as_numpy_iterator()\n",
        "  for val_example in val_iterator:\n",
        "    eval_loss += compiled_validation_step(model,\n",
        "                                          params,\n",
        "                                          dataset_builder._tokenizer.pad_id,\n",
        "                                          val_example)\n",
        "    n_steps_eval += 1\n",
        "  print(f\"Start, validation loss: {eval_loss/n_steps_eval}\")\n",
        "\n",
        "  for train_example in train_ds:\n",
        "    train_loss, params, opt_state = compiled_train_step(model=model,\n",
        "                                                        params=params,\n",
        "                                                        optimizer=optimizer,\n",
        "                                                        opt_state=opt_state,\n",
        "                                                        pad_id=dataset_builder._tokenizer.pad_id,\n",
        "                                                        example=train_example)\n",
        "    n_steps += 1\n",
        "    avg_loss += train_loss\n",
        "    if n_steps % training_cfg.eval_every_n == 0:\n",
        "      eval_loss = 0\n",
        "\n",
        "      n_steps_eval = 0\n",
        "      val_iterator = validation_ds.as_numpy_iterator()\n",
        "      for val_example in val_iterator:\n",
        "        eval_loss += compiled_validation_step(model,\n",
        "                                              params,\n",
        "                                              dataset_builder._tokenizer.pad_id,\n",
        "                                              val_example)\n",
        "        n_steps_eval +=1\n",
        "      avg_loss /= training_cfg.eval_every_n\n",
        "      eval_loss /= n_steps_eval\n",
        "      print(f\"STEP {n_steps} training loss: {avg_loss} - eval loss: {eval_loss}\")\n",
        "      avg_loss=0\n",
        "    if training_cfg.max_steps is not None and n_steps > training_cfg.max_steps:\n",
        "      break\n",
        "  return params"
      ],
      "metadata": {
        "id": "FPT-0pQTeRBH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Begin fine-tuning the Gemma model on a limited number of steps (SEQ_SIZE) to make sure this fits in the memory\n",
        "SEQ_SIZE = 25\n",
        "tokenizer = GemmaTokenizer(vocab)\n",
        "dataset_builder= MTNTDatasetBuilder(tokenizer, SEQ_SIZE)\n",
        "training_cfg = TrainingConfig(learning_rate=1e-4,\n",
        "                              num_epochs=1,\n",
        "                              eval_every_n=20,\n",
        "                              batch_size=1,\n",
        "                              max_steps=100)\n",
        "\n",
        "params = train_loop(model=model_2b,\n",
        "                    params={'params': params['transformer']},\n",
        "                    dataset_builder=dataset_builder,\n",
        "                    training_cfg=training_cfg)"
      ],
      "metadata": {
        "id": "icFSamJsemUC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a sampler with gemma.sampler.Sampler. It uses the Gemma model checkpoint and the tokenizer\n",
        "sampler = sampler_lib.Sampler(\n",
        "    transformer=model_2b,\n",
        "    vocab=vocab,\n",
        "    params=params['params'],\n",
        ")\n"
      ],
      "metadata": {
        "id": "oSPLYH4pfGEz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampler(\n",
        "    [\"Translate this into French:\\nHello, my name is Morgane.\\n\"],\n",
        "    total_generation_steps=100,\n",
        "    ).text"
      ],
      "metadata": {
        "id": "AKIcqfMJgVYK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}